#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºFAISSçš„RAGç³»ç»Ÿ
éœ€è¦å®‰è£…: pip install faiss-cpu sentence-transformers ollama numpy
"""

import ollama
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import pickle
import os
from typing import List, Dict, Any, Tuple
import json

class FaissRAGSystem:
    def __init__(self, 
                 ollama_host: str = "http://10.130.149.18:11434",
                 model_name: str = "qwen2.5-coder:14b",
                 embedding_model: str = "all-MiniLM-L6-v2",
                 index_file: str = "./faiss_index"):
        """
        åŸºäºFAISSçš„RAGç³»ç»Ÿ
        
        Args:
            ollama_host: OllamaæœåŠ¡å™¨åœ°å€
            model_name: ç”Ÿæˆæ¨¡å‹åç§°
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            index_file: FAISSç´¢å¼•æ–‡ä»¶è·¯å¾„å‰ç¼€
        """
        self.ollama_host = ollama_host
        self.model_name = model_name
        self.embedding_model_name = embedding_model
        self.index_file = index_file
        
        # å­˜å‚¨æ–‡æ¡£çš„æ•°æ®ç»“æ„
        self.documents = []  # å­˜å‚¨åŸå§‹æ–‡æ¡£
        self.metadatas = []  # å­˜å‚¨å…ƒæ•°æ®
        self.doc_ids = []    # å­˜å‚¨æ–‡æ¡£ID
        
        print("=" * 60)
        print("åˆå§‹åŒ–FAISS-RAGç³»ç»Ÿ")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_ollama()
        self._init_embedding_model()
        self._init_faiss_index()
        
        print("âœ… FAISS-RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    
    def _init_ollama(self):
        """åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯"""
        try:
            print(f"ğŸ”— è¿æ¥OllamaæœåŠ¡å™¨: {self.ollama_host}")
            self.ollama_client = ollama.Client(host=self.ollama_host)
            
            # æµ‹è¯•è¿æ¥
            response = self.ollama_client.list()
            print(f"âœ… Ollamaè¿æ¥æˆåŠŸ")
            
            # æ£€æŸ¥æ¨¡å‹
            model_found = False
            if 'models' in response:
                for model in response['models']:
                    name = model.get('name', model.get('model', ''))
                    if self.model_name in name:
                        model_found = True
                        break
            
            if model_found:
                print(f"âœ… æ¨¡å‹ {self.model_name} å¯ç”¨")
            else:
                print(f"âš ï¸  æ¨¡å‹ {self.model_name} æœªæ‰¾åˆ°")
                
        except Exception as e:
            print(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}")
            raise
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            print(f"ğŸ“š åŠ è½½åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            
            # è·å–åµŒå…¥ç»´åº¦
            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            print(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.embedding_dim}")
            
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _init_faiss_index(self):
        """åˆå§‹åŒ–FAISSç´¢å¼•"""
        try:
            print("ğŸ—„ï¸  åˆå§‹åŒ–FAISSç´¢å¼•...")
            
            # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
            if self._load_index():
                print("âœ… åŠ è½½ç°æœ‰FAISSç´¢å¼•")
            else:
                # åˆ›å»ºæ–°ç´¢å¼•
                print("åˆ›å»ºæ–°çš„FAISSç´¢å¼•...")
                # ä½¿ç”¨L2è·ç¦»çš„å¹³é¢ç´¢å¼•
                self.index = faiss.IndexFlatL2(self.embedding_dim)
                print("âœ… æ–°FAISSç´¢å¼•åˆ›å»ºæˆåŠŸ")
                
        except Exception as e:
            print(f"âŒ FAISSç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _load_index(self) -> bool:
        """åŠ è½½ç°æœ‰çš„FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            index_path = f"{self.index_file}.index"
            metadata_path = f"{self.index_file}.pkl"
            
            if os.path.exists(index_path) and os.path.exists(metadata_path):
                # åŠ è½½FAISSç´¢å¼•
                self.index = faiss.read_index(index_path)
                
                # åŠ è½½å…ƒæ•°æ®
                with open(metadata_path, 'rb') as f:
                    data = pickle.load(f)
                    self.documents = data['documents']
                    self.metadatas = data['metadatas']
                    self.doc_ids = data['doc_ids']
                
                print(f"ğŸ“Š åŠ è½½äº† {len(self.documents)} ä¸ªæ–‡æ¡£")
                return True
                
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
        
        return False
    
    def save_index(self):
        """ä¿å­˜FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            # ä¿å­˜FAISSç´¢å¼•
            index_path = f"{self.index_file}.index"
            faiss.write_index(self.index, index_path)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_path = f"{self.index_file}.pkl"
            with open(metadata_path, 'wb') as f:
                pickle.dump({
                    'documents': self.documents,
                    'metadatas': self.metadatas,
                    'doc_ids': self.doc_ids
                }, f)
            
            print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜åˆ° {self.index_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def add_document(self, text: str, doc_id: str, metadata: dict = None):
        """æ·»åŠ æ–‡æ¡£åˆ°FAISSç´¢å¼•"""
        try:
            print(f"ğŸ“„ æ·»åŠ æ–‡æ¡£: {doc_id}")
            
            # åˆ†å—å¤„ç†é•¿æ–‡æ¡£
            chunks = self._chunk_text(text, chunk_size=500)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"
                
                # ç”ŸæˆåµŒå…¥å‘é‡
                embedding = self.embedding_model.encode([chunk])
                embedding = embedding.astype('float32')  # FAISSéœ€è¦float32
                
                # æ·»åŠ åˆ°FAISSç´¢å¼•
                self.index.add(embedding)
                
                # å­˜å‚¨æ–‡æ¡£å’Œå…ƒæ•°æ®
                chunk_metadata = metadata or {}
                chunk_metadata.update({
                    'source_doc': doc_id,
                    'chunk_index': i,
                    'chunk_id': chunk_id,
                    'text_length': len(chunk)
                })
                
                self.documents.append(chunk)
                self.metadatas.append(chunk_metadata)
                self.doc_ids.append(chunk_id)
            
            print(f"âœ… æ–‡æ¡£ {doc_id} æ·»åŠ å®Œæˆ ({len(chunks)} ä¸ªç‰‡æ®µ)")
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
    
    def _chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """æ–‡æœ¬åˆ†å—"""
        if len(text) <= chunk_size:
            return [text.strip()]
        
        # æŒ‰å¥å­åˆ†å‰²ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        sentences = text.replace('ã€‚', 'ã€‚\n').replace('ï¼', 'ï¼\n').replace('ï¼Ÿ', 'ï¼Ÿ\n').split('\n')
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) <= chunk_size:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def search_documents(self, query: str, k: int = 3) -> List[Tuple[str, float, dict]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        try:
            if self.index.ntotal == 0:
                print("âš ï¸  ç´¢å¼•ä¸­æ²¡æœ‰æ–‡æ¡£")
                return []
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query])
            query_embedding = query_embedding.astype('float32')
            
            # åœ¨FAISSä¸­æœç´¢
            distances, indices = self.index.search(query_embedding, k)
            
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx >= 0 and idx < len(self.documents):
                    similarity = 1 / (1 + distance)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    results.append((
                        self.documents[idx],
                        similarity,
                        self.metadatas[idx]
                    ))
            
            return results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def ask(self, question: str, k: int = 3) -> str:
        """RAGé—®ç­”"""
        try:
            print(f"ğŸ¤” é—®é¢˜: {question}")
            print(f"ğŸ” åœ¨FAISSç´¢å¼•ä¸­æœç´¢ç›¸å…³æ–‡æ¡£...")
            
            # æœç´¢ç›¸å…³æ–‡æ¡£
            search_results = self.search_documents(question, k)
            
            if not search_results:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
            
            print(f"ğŸ“– æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_docs = [doc for doc, score, metadata in search_results]
            context = "\n\n".join(context_docs)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·ç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
            
            print(f"ğŸ¤– ä½¿ç”¨ {self.model_name} ç”Ÿæˆå›ç­”...")
            
            # ç”Ÿæˆå›ç­” - æ·»åŠ å†…å­˜ä¼˜åŒ–é€‰é¡¹
            response = self.ollama_client.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    'temperature': 0.3,
                    'top_p': 0.8,
                    'num_ctx': 2048,  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                    'num_predict': 512,  # é™åˆ¶è¾“å‡ºé•¿åº¦
                    'num_thread': 4,  # é™åˆ¶çº¿ç¨‹æ•°
                }
            )
            
            return response['response']
            
        except Exception as e:
            if "CUDA error: out of memory" in str(e):
                return f"âš ï¸  GPUå†…å­˜ä¸è¶³ï¼Œå»ºè®®ï¼š\n1. å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦\n2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹\n3. åˆ‡æ¢åˆ°CPUæ¨¡å¼\n\næ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š\n{context}"
            return f"å›ç­”ç”Ÿæˆå¤±è´¥: {e}"
    
    def chat(self, message: str) -> str:
        """ç®€å•å¯¹è¯"""
        try:
            response = self.ollama_client.generate(
                model=self.model_name,
                prompt=message,
                options={
                    'num_ctx': 1024,  # å‡å°‘ä¸Šä¸‹æ–‡
                    'num_predict': 256,  # å‡å°‘è¾“å‡ºé•¿åº¦
                }
            )
            return response['response']
        except Exception as e:
            return f"å¯¹è¯å¤±è´¥: {e}"
    
    def get_stats(self):
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_documents': len(self.documents),
            'index_size': self.index.ntotal,
            'embedding_dimension': self.embedding_dim
        }


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        # åˆå§‹åŒ–FAISS-RAGç³»ç»Ÿ
        rag = FaissRAGSystem()
        
        # å‡†å¤‡æµ‹è¯•æ–‡æ¡£
        test_documents = {
            "python_basics": """
            Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´æ˜äº†çš„è¯­æ³•ã€‚
            å®ƒæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚
            Pythonåœ¨æ•°æ®ç§‘å­¦ã€Webå¼€å‘ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚
            å…¶å¼ºå¤§çš„æ ‡å‡†åº“å’Œä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åŒ…ä½¿å¾—å¼€å‘æ•ˆç‡å¾ˆé«˜ã€‚
            Pythonçš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç çš„å¯è¯»æ€§å’Œç®€æ´æ€§ã€‚
            """,
            
            "machine_learning": """
            æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚
            å®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
            å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰ã€‚
            æœºå™¨å­¦ä¹ åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚
            ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®ï¼Œæ— ç›‘ç£å­¦ä¹ å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ã€‚
            å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚
            """,
            
            "deep_learning": """
            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚
            å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸè¡¨ç°çªå‡ºã€‚
            æ·±åº¦å­¦ä¹ æ¨¡å‹åŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œã€transformerç­‰ã€‚
            æ·±åº¦å­¦ä¹ çš„æˆåŠŸå¾—ç›Šäºå¤§æ•°æ®ã€å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›å’Œç®—æ³•æ”¹è¿›ã€‚
            å®ƒèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤ºï¼Œå‡å°‘äº†äººå·¥ç‰¹å¾å·¥ç¨‹çš„éœ€æ±‚ã€‚
            """
        }
        
        print("\n" + "=" * 60)
        print("æ·»åŠ æµ‹è¯•æ–‡æ¡£åˆ°FAISSç´¢å¼•")
        print("=" * 60)
        
        # æ·»åŠ æ–‡æ¡£
        for doc_id, content in test_documents.items():
            rag.add_document(
                text=content.strip(),
                doc_id=doc_id,
                metadata={"category": "æŠ€æœ¯æ–‡æ¡£", "source": "test"}
            )
        
        # ä¿å­˜ç´¢å¼•
        rag.save_index()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = rag.get_stats()
        print(f"\nğŸ“Š ç´¢å¼•ç»Ÿè®¡:")
        print(f"  æ–‡æ¡£ç‰‡æ®µæ•°: {stats['total_documents']}")
        print(f"  ç´¢å¼•å¤§å°: {stats['index_size']}")
        print(f"  åµŒå…¥ç»´åº¦: {stats['embedding_dimension']}")
        
        print("\n" + "=" * 60)
        print("FAISS-RAGé—®ç­”æµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "Pythonæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
            "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¸»è¦ç±»å‹ï¼Ÿ", 
            "æ·±åº¦å­¦ä¹ åœ¨å“ªäº›é¢†åŸŸè¡¨ç°çªå‡ºï¼Ÿ",
            "ä»€ä¹ˆæ˜¯ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ï¼Ÿ"
        ]
        
        for question in test_questions:
            print(f"\n{'='*50}")
            answer = rag.ask(question, k=2)  # å‡å°‘æ£€ç´¢æ–‡æ¡£æ•°é‡
            print(f"ğŸ’¬ å›ç­”: {answer}")
        
        print("\n" + "=" * 60)
        print("æœç´¢æµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        search_query = "ç¥ç»ç½‘ç»œ"
        results = rag.search_documents(search_query, k=3)
        print(f"æœç´¢æŸ¥è¯¢: {search_query}")
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:")
        
        for i, (doc, score, metadata) in enumerate(results, 1):
            print(f"\n{i}. ç›¸ä¼¼åº¦: {score:.3f}")
            print(f"   æ¥æº: {metadata.get('source_doc', 'unknown')}")
            print(f"   å†…å®¹: {doc[:100]}...")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()